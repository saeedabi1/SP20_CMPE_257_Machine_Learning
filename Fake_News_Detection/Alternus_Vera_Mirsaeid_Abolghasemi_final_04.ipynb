{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Alternus_Vera_Mirsaeid_Abolghasemi_final_04.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p5vMIdDPAo9y"},"source":["# Mirsaeid Abolghasemi - Machine Learning final\n","\n","I ran some cells on different Google accounts and merge them here, because my Colab crashed a lot after running the whole code.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"enGfRjKqBH9Z"},"source":["# Alternus Vera \n","\n","Course code : **CMPE-257** \n","    \n","\n","    \n","Name:  **Mirsaeid Abolghasemi**  \n","\n","-----\n","\n","\n","### Liar Liar Pants on Fire Dataset Description \n","- It has 3 files test, training and valid.\n","- Each file has 14 columns\n","    \n","    Column 1: the ID of the statement ([ID].json).\n","    \n","    Column 2: the label.\n","    \n","    Column 3: the statement.\n","    \n","    Column 4: the subject(s).\n","    \n","    Column 5: the speaker.\n","    \n","    Column 6: the speaker's job title.\n","    \n","    Column 7: the state info.\n","    \n","    Column 8: the party affiliation.\n","    \n","    Column 9-13: the total credit history count, including the current statement.\n","    \n","    Column 14: the context (venue / location of the speech or statement).\n","\n","### Process of My Approach \n","- Load the Data\n","- Distillation Process\n","    - Data Cleaning and Text Preprocessing\n","    - Visualization\n","- **Feature 1 :** Corpus Structure\n","- Vector Classification Modeling \n","- Ranking and Importance\n","- Merge all features and individual contributions\n","- Form Polynomial Equation \n","    \n","\n","### Feature Selection\n","**List top Features Selected based on research articles**\n","\n","\n","\n","### The Team Contributions:\n","\n","|Features  |  Member |\n","|-----|-----|\n","| Psychology utility                         |  Varun Bhaseen |  \n","| Content statistics                 |  Gulya Timokhina | \n","| ClickBait                   |  Poornapragna Vadiraj  | \n","| Corpus Structure                   |  Mirsaeid Abolghasemi  |   \n","\n"," \n","#### Enrichment Dataset Details\n","\n","- Dataset for our real news data is https://www.kaggle.com/snapcrack/all-the-news#articles1.csv\n","- Dataset for Fake news data used the Kaggle Fake News Data Set (https://www.kaggle.com/mrisdal/fake-news)\n","\n","#### Libraries Used \n","\n","- NLTK \n","- Gensim \n","- Numpy\n","- Pandas\n","- CSV\n","- WordCloud\n","- Seaborn\n","- Scipy\n","- Regualr Expression\n","- Matplotlib\n","- Sklearn \n","\n","\n","#### What did I try and What worked? \n","\n","> I tried to add Unigram, Bigrams, Tri-grams, 4-grams from cleaned material for the corpus structure function, vectorizing the documents with TF-IDF, then implementing multiple classification algorithms such as XGBoost, Random Forest, and SVM. Compare for each of the F1 ratings and  accuracies. I have experimented around with the C values transferred in the test, which also makes me minor changes on the accuracy ranking, by vectorizing each text based on ngrams.\n","\n","#### What did not work?\n","\n","> While ngrams provided the distinction on the accuracy of the model, however with the main liar-liar dataset, the accuracy of the model did not have so much change, it ranged from 50 percent to 58 percent, while I used various classification algorithms, the accuracy of the model did not increase so much.\n","\n","\n","#### What alternatives did you try?\n","\n","> I attempted data amalgamation by collecting extra actual news data and false news from kaggle, cleaning feature material and text columns by adding and equivalent cleaning logic used in the actual data collection, enriching it with the real dataset and feeding it back into the same model.\n","----"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CNjcIYzBAn8x","scrolled":true,"colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import csv\n","#import gensim\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import StandardScaler\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import confusion_matrix\n","from nltk.stem.porter import PorterStemmer\n","from sklearn import metrics\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.pipeline import Pipeline\n","from nltk.corpus import stopwords\n","from string import punctuation\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","\n","import matplotlib.pyplot as plt\n","from scipy import sparse\n","# Code source: https://degravek.github.io/project-pages/project1/2017/04/28/New-Notebook/\n","# Dataset from Chakraborty et al. (https://github.com/bhargaviparanjape/clickbait/tree/master/dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mRtcaG4aXOlK","colab":{}},"source":["# Import PyDrive and associated libraries.\n","# This only needs to be done once per notebook.\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# Download a file based on its file ID.\n","#\n","# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n","\n","file_id = '1E3-see3z8mcsa8PdAkbx8S-luv6v7pkQ'\n","downloaded = drive.CreateFile({'id': file_id})\n","#print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n","downloaded.GetContentFile('test.tsv')\n","\n","\n","file_id = '1ZGVezZTHqFzpsUcSWn-wCuEqYMDUh0oq'\n","downloaded = drive.CreateFile({'id': file_id})\n","#print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n","downloaded.GetContentFile('train.tsv')\n","\n","file_id = '1bM-cgvmjW8yhP23EQLgOMRyON8R6w_V5'\n","downloaded = drive.CreateFile({'id': file_id})\n","#print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n","downloaded.GetContentFile('valid.tsv')\n","\n","\n","# Content articles1.csv - 50,000 news articles (Articles 1-50,000)\n","file_id = '1fau5ZGMP47VInKy1V5xfhjID5D9Nq87Y'\n","downloaded = drive.CreateFile({'id': file_id})\n","#print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n","downloaded.GetContentFile('articles1.csv')\n","\n","\n","file_id = '1AbayLqq7kxY_UykkLr5zEoJFB8mvT9lS'\n","downloaded = drive.CreateFile({'id': file_id})\n","#print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n","downloaded.GetContentFile('fake.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DJO_BPuiON-z","colab":{}},"source":["# Read the test, training and valid data from files\n","# Header = 0 indicates that the first line of the file contains column names,\n","# As there is no Header, create a column names for each column in the dataset\n","# delimiter = \\t indicates that the fields are seperated by tabs, and \n","\n","\n","test_filename = 'test.tsv'\n","train_filename = 'train.tsv'\n","valid_filename = 'valid.tsv'\n","\n","colnames = ['jsonid', 'label', 'headline_text', 'subject', 'speaker', 'speakerjobtitle', 'stateinfo','partyaffiliation', 'barelytruecounts', 'falsecounts','halftruecounts','mostlytrueocunts','pantsonfirecounts','context']\n","\n","train_news = pd.read_csv(train_filename, sep='\\t', names = colnames, error_bad_lines=False)\n","test_news = pd.read_csv(test_filename, sep='\\t', names = colnames, error_bad_lines=False)\n","valid_news = pd.read_csv(valid_filename, sep='\\t', names = colnames, error_bad_lines=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTSOm6e6_A2-","colab_type":"code","colab":{}},"source":["# train_news = train_news[:1000]\n","# test_news = test_news[:1000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IRVLFDAoOOBt","colab":{}},"source":["# Display check the dimensions and the first 2 rows of the file.\n","\n","print('train dim:',train_news.shape, 'test dim:', test_news.shape)\n","train_news.iloc[0:2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BHP2SZXoOOHB","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gjO0rLQCIyX9"},"source":["## Data Cleaning and Text Preprocessing \n","\n","*Steps included in the preprocessing:*\n","- Remove Special Characters and Punctuations\n","- Lower case the news\n","- Tokenization\n","- Remove Stop Words\n","- Lemmatization\n","- Stemming \n","- Spell Check "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8bmAeNDdOOJv","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7d_1XelRI1BA"},"source":["###  Putting It All Together \n","\n","To make the code reusable, we need to create a function that can be called many times."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_FCziRuJOOMT","colab":{}},"source":["import re\n","\n","def cleaning(raw_news):\n","    import nltk\n","    \n","    # 1. Remove non-letters/Special Characters and Punctuations\n","    news = re.sub(\"[^a-zA-Z]\", \" \", str(raw_news))\n","    \n","    # 2. Convert to lower case.\n","    news =  news.lower()\n","    \n","    # 3. Tokenize.\n","    news_words = nltk.word_tokenize( news)\n","    \n","    # 4. Convert the stopwords list to \"set\" data type.\n","    stops = set(nltk.corpus.stopwords.words(\"english\"))\n","    \n","    # 5. Remove stop words. \n","    words = [w for w in  news_words  if not w in stops]\n","    \n","    # 6. Lemmentize \n","    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n","    \n","    # 7. Stemming\n","    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n","    \n","    # 8. Join the stemmed words back into one string separated by space, and return the result.\n","    return \" \".join(stems)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tWnGWB_yOOPK","colab":{}},"source":["import nltk\n","nltk.download('wordnet')\n","\n","nltk.download('punkt')\n","\n","#SENT_DETECTOR = nltk.data.load('tokenizers/punkt/english.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"u-RV1hQtOORz","colab":{}},"source":["\n","import time\n","# clean training and test data \n","# create new column \"tokenized\"\n","t1 = time.time()\n","\n","# Add the processed data to the original data. \n","# Perhaps using apply function would be more elegant and concise than using for loop\n","train_news['clean'] = train_news[\"headline_text\"].apply(cleaning) \n","\n","t2 = time.time()\n","print(\"\\nTime to clean, tokenize and stem train data: \\n\", len(train_news), \"news:\", (t2-t1)/60, \"min\")\n","\n","t1 = time.time()\n","test_news['clean'] = test_news[\"headline_text\"].apply(cleaning)\n","\n","t2 = time.time()\n","print(\"\\n\\nTime to clean, tokenize and stem test data: \\n\", len(test_news), \"news:\", (t2-t1)/60, \"min\")\n","\n","t1 = time.time()\n","valid_news['clean'] = valid_news[\"headline_text\"].apply(cleaning)\n","\n","t2 = time.time()\n","print(\"\\n\\nTime to clean, tokenize and stem valid data: \\n\", len(valid_news), \"news:\", (t2-t1)/60, \"min\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"L_yupyfGOOUl","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OqLR3c7pOOXL","colab":{}},"source":["# file_id = '1W9Aj0S8Pp8MC13dwbjTSeWPeLn9m-RNP'\n","# downloaded = drive.CreateFile({'id': file_id})\n","# #print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n","# downloaded.GetContentFile('GoogleNews-vectors-negative300.bin.gz')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5PV63-HhJSnv"},"source":["### [Google News corpus word2vec](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/)\n","\n","### Spell Check \n","\n","-  You can download the pre-trained model [**here**](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)\n","\n","- Or clone it from GitHub [**GoogleNews-vectors-negative300**](https://github.com/mmihaltz/word2vec-GoogleNews-vectors)\n","\n","> It’s 1.5GB! It includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features.\n","\n","**3 million words * 300 features * 4bytes/feature = ~3.35GB**\n","\n","> This file consist of the word2vec -  pre-trained Google News corpus (3 billion running words) to word vector model (3 million 300-dimension English word vectors).\n","\n","> Look at the [**vocabulory list**](https://github.com/chrisjmccormick/inspect_word2vec/tree/master/vocabulary) used to train this model. Each text file contains 100,000 entries from the model. \n","\n","\n",">  There are few things that this dataset contains and not. It has stop words like  “the”, “also”, “should” and does not have stop words like “a”, “and”, “of”. As I have removed the stop words the complexity is reduced as there is no need to check the spelling for stop words. \n","\n","> It does have numbers but in the form of entried wiht #. e.g., you won’t find “100”. But it does include entries like “###MHz_DDR2_SDRAM”. \n","\n","The model used [**WinPython-64bit-2.7.10.3**](https://winpython.github.io/) for efficient python distribution on Windows system. Helps to run the scripts in batches. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HFjZE4BcOOZ4","colab":{}},"source":["# model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n","# words = model.index2word\n","\n","# w_rank = {}\n","# for i,word in enumerate(words):\n","#     w_rank[word] = i\n","\n","# WORDS = w_rank"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BqmTEB-GOOc2","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XSq458HHkFxh","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xi3yn-LbkF0E","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M4BCTP17kF2y","colab":{}},"source":["# import re\n","# from collections import Counter\n","\n","# def words(text): return re.findall(r'\\w+', text.lower())\n","\n","# def P(word, N=sum(WORDS.values())): \n","#     \"Probability of `word`.\"\n","#     return - WORDS.get(word, 0)\n","\n","# def correction(word): \n","#     \"Most probable spelling correction for word.\"\n","#     return max(candidates(word), key=P)\n","\n","# def candidates(word): \n","#     \"Generate possible spelling corrections for word.\"\n","#     return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n","\n","# def known(words): \n","#     \"The subset of `words` that appear in the dictionary of WORDS.\"\n","#     return set(w for w in words if w in WORDS)\n","\n","# def edits1(word):\n","#     \"All edits that are one edit away from `word`.\"\n","#     letters    = 'abcdefghijklmnopqrstuvwxyz'\n","#     splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","#     deletes    = [L + R[1:]               for L, R in splits if R]\n","#     transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","#     replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","#     inserts    = [L + c + R               for L, R in splits for c in letters]\n","#     return set(deletes + transposes + replaces + inserts)\n","\n","# def edits2(word): \n","#     \"All edits that are two edits away from `word`.\"\n","#     return (e2 for e1 in edits1(word) for e2 in edits1(e1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4hu_xPjblYeE","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uHcXkG_OlYg-","colab":{}},"source":["# def spell_checker(text):\n","#     all_words = re.findall(r'\\w+', text.lower()) # split sentence to words\n","#     spell_checked_text  = []\n","#     for i in range(len(all_words)):\n","#         spell_checked_text.append(correction(all_words[i]))\n","#     return ' '.join(spell_checked_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xW2qwXJulzgC","colab":{}},"source":["# test_news['clean']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-H9PgeFZlYj6","colab":{}},"source":["# print(\"Before: \\n\", train_news['clean'][0] )\n","# t1 = time.time()\n","# train_news['clean'] = train_news['clean'].apply(spell_checker)\n","# t2 = time.time()\n","# print(\"\\nTime to spell check the train data: \\n\", len(train_news), \"news:\", (t2-t1)/60, \"min\")\n","\n","# print(\"\\nAfter: \\n\",train_news['clean'][0] )\n","# train_news.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XP593Qt6lYmo","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0kKtvCGrlYpe","colab":{}},"source":["# t1 = time.time()\n","# test_news['clean'] = test_news['clean'].apply(spell_checker)\n","# test_news.head(5)\n","# t2 = time.time()\n","# print(\"\\nTime to spell check the test data: \\n\", len(test_news), \"news:\", (t2-t1)/60, \"min\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Mgbk-kpAlYr-","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XyHNMf1Rng3J","colab":{}},"source":["# t1 = time.time()\n","# valid_news['clean'] = valid_news['clean'].apply(spell_checker)\n","# valid_news.head(5)\n","# t2 = time.time()\n","# print(\"\\nTime to spell check the valid data: \\n\", len(valid_news), \"news:\", (t2-t1)/60, \"min\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6kBGv83dng6L","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4RTPBS6sng9B","colab":{}},"source":["# train_news.to_csv(\"train_processed.csv\", sep=',')\n","# test_news.to_csv(\"test_processed.csv\", sep=',')\n","# valid_news.to_csv(\"valid_processed.csv\", sep=',')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1_hX6WiLoV9n"},"source":["# Visualization \n","\n","#### WordCloud \n","\n","> As a tool for visualization by using the frequency of words appeared in text, we use WordCloud. Note that it can give more information and insight of texts by analyzing correlations and similarities between words rather than analyzing texts only by the frequency of words appeared; however, it can give you some general shape of what this text is about quickly and intuitively."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D86MYjKYng_2","colab":{}},"source":["# from wordcloud import WordCloud, STOPWORDS\n","# import matplotlib.pyplot as plt\n","# import seaborn as sns\n","# from scipy import stats\n","\n","# %matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"v2hDO-IqnhDH","colab":{}},"source":["# def cloud(data,backgroundcolor = 'white', width = 800, height = 600):\n","#     wordcloud = WordCloud(stopwords = STOPWORDS, background_color = backgroundcolor,\n","#                          width = width, height = height).generate(data)\n","#     plt.figure(figsize = (15, 10))\n","#     plt.imshow(wordcloud)\n","#     plt.axis(\"off\")\n","#     plt.show()\n","    \n","# cloud(' '.join(train_news['clean']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MbhWvmj6nhGR","colab":{}},"source":["#!conda install -c conda-forge xgboost"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FnELUcrvo2n1"},"source":["# Feature:Corpus Structure"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1UXvr_AYnhJV","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","\n","from sklearn import svm\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","import xgboost as xgb\n","from sklearn.metrics import mean_squared_error\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report\n","from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gInLQ04gZ_YZ"},"source":["## creating a class of the feature \"CorpusStructure\" and defining the fucntions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dLChpA7jnhRx","colab":{}},"source":["# creating a class of the feature \"CorpusStructure\" and defining the fucntions\n","# we need to use for 3 classifiers XGBoost, SVM, and Random Forest\n","class CorpusStructure():\n","    \n","    def __init__(self):\n","\n","        self.xgboost_pipeline = ''\n","        self.svm_pipeline = ''\n","        self.logisticRegression_pipeline = ''\n","        self.randomForest_pipeline = ''\n","        \n","        self.xgboost_enrich_pipeline = ''\n","        self.svm_enrich_pipeline = ''\n","        self.logisticRegression_enrich_pipeline = ''\n","        self.randomForest_enrich_pipeline = ''\n","    \n","    # function of multi output label\n","    def multiOutputLabel(self, r):\n","        v = r['label']\n","\n","        # catagorize 'true', 'mostly-true', 'half-true','barely-true', 'false', 'pants-fire'\n","        if (v == 'true'):\n","            return 5\n","        if (v == 'mostly-true'):\n","            return 4\n","        if (v == 'half-true'):\n","            return 3\n","        if (v == 'barely-true'):\n","            return 2\n","        if (v == 'false'):\n","            return 1\n","        if (v == 'pants-fire'):\n","            return 0\n","\n","    # Piplines \n","    # XGBoost Pipline ***************************************************\n","    def xgboost_pipline(self, ngram_input_range=(1,1)):\n","                \n","        tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=ngram_input_range)\n","        count_vectorizer = CountVectorizer(ngram_range=ngram_input_range)\n","        \n","        X = count_vectorizer.fit_transform(train_news['clean'])\n","\n","        \n","        X_train = tfidf_vectorizer.fit_transform(train_news['clean'].values)\n","        X_test = tfidf_vectorizer.transform(test_news['clean'].values)\n","        \n","        train_news['multiLabel'] = train_news.apply(lambda row: self.multiOutputLabel(row), axis=1)\n","        test_news['multiLabel'] = test_news.apply(lambda row: self.multiOutputLabel(row), axis=1)\n","\n","        y_train = train_news['multiLabel']\n","        y_test = test_news['multiLabel']\n","\n","        # print(\"\\ny_train:\",y_train.shape)\n","        # print(\"y_train:\",y_test.shape)\n","        # print(\"\\ny_train:\",y_train)\n","        # print(\"y_train:\",y_test)\n","        \n","        self.xgboost_pipeline = Pipeline([\n","            ('vectorizer', tfidf_vectorizer),\n","            ('clf', xgb.XGBClassifier(random_state=1,learning_rate=0.01))\n","            ])\n","\n","\n","        self.xgboost_pipeline.fit(train_news['clean'].values, y_train)\n","        predicts = self.xgboost_pipeline.predict(test_news['clean'].values)\n","        score = metrics.accuracy_score(y_test, predicts)\n","\n","\n","\n","\n","        print(\"Accuracy:\",score)\n","        print(\"\\n\")\n","        print(classification_report(y_test, predicts))\n","        return score\n","\n","    # XGBoost Predict\n","    def xgboost_predict(self, text):\n","        predicted = self.xgboost_pipeline.predict([text])\n","        predicedProb = self.xgboost_pipeline.predict_proba([text])[:,1]\n","        return predicted, float(predicedProb)\n","\n","\n","    # SVM Pipline ***************************************************\n","    def svm_pipline(self, ngram_input_range=(1,1)):\n","                \n","        tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=ngram_input_range)\n","        count_vectorizer = CountVectorizer(ngram_range=ngram_input_range)\n","        \n","        X = count_vectorizer.fit_transform(train_news['clean'])\n","\n","        \n","        X_train = tfidf_vectorizer.fit_transform(train_news['clean'].values)\n","        X_test = tfidf_vectorizer.transform(test_news['clean'].values)\n","        \n","        train_news['multiLabel'] = train_news.apply(lambda row: self.multiOutputLabel(row), axis=1)\n","        test_news['multiLabel'] = test_news.apply(lambda row: self.multiOutputLabel(row), axis=1)\n","\n","        y_train = train_news['multiLabel']\n","        y_test = test_news['multiLabel']\n","        \n","        self.svm_pipeline = Pipeline([\n","            ('vectorizer', tfidf_vectorizer),\n","            ('clf', svm.SVC(probability=True))\n","            ])\n","        \n","        self.svm_pipeline.fit(train_news['clean'].values, y_train)\n","        predicts = self.svm_pipeline.predict(test_news['clean'].values)\n","        score = metrics.accuracy_score(y_test, predicts)\n","\n","        print(\"Accuracy:\",score)\n","        print(\"\\n\")\n","        print(classification_report(y_test, predicts))\n","        return score\n","\n","    def svm_predict(self, text):\n","        predicted = self.svm_pipeline.predict([text])\n","        predicedProb = self.svm_pipeline.predict_proba([text])[:,1]\n","        return predicted, float(predicedProb)\n","\n","    \n","    # Random Forest Pipline ***************************************************\n","    def rf_pipline(self, ngram_input_range=(1,1)):\n","                \n","        tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=ngram_input_range)\n","        count_vectorizer = CountVectorizer(ngram_range=ngram_input_range)\n","        \n","        X = count_vectorizer.fit_transform(train_news['clean'])\n","\n","        \n","        X_train = tfidf_vectorizer.fit_transform(train_news['clean'].values)\n","        X_test = tfidf_vectorizer.transform(test_news['clean'].values)\n","        \n","        train_news['multiLabel'] = train_news.apply(lambda row: self.multiOutputLabel(row), axis=1)\n","        test_news['multiLabel'] = test_news.apply(lambda row: self.multiOutputLabel(row), axis=1) \n","\n","        y_train = train_news['multiLabel']\n","        y_test = test_news['multiLabel']\n","        \n","        self.randomForest_pipeline = Pipeline([\n","            ('vectorizer', tfidf_vectorizer),\n","            ('clf', RandomForestClassifier(n_estimators = 300))\n","            ])\n","\n","\n","\n","\n","        self.randomForest_pipeline.fit(train_news['clean'].values, y_train)\n","        predicts = self.randomForest_pipeline.predict(test_news['clean'].values)\n","        score = metrics.accuracy_score(y_test, predicts)\n","        print(\"Accuracy:\",score)\n","        print(\"\\n\")\n","        print(classification_report(y_test, predicts))\n","        return score\n","\n","    def rf_predict(self, text):\n","        predicted = self.randomForest_pipeline.predict([text])\n","        predicedProb = self.randomForest_pipeline.predict_proba([text])[:,1]\n","        return predicted, float(predicedProb)\n","\n","\n","    # data enrichment with another dataset   *****************************\n","    # Content articles1.csv - 50,000 news articles (Articles 1-50,000)\n","    def data_enrichment(self):\n","\n","        df_articles = pd.read_csv(\"articles1.csv\")\n","        df_articles.head(3)\n","        \n","        publications = np.unique(df_articles[\"publication\"].values)\n","        print(\"publications:\\n\")\n","        print(publications)\n","        \n","        df_articles = df_articles.loc[df_articles['publication'].isin(['New York Times', 'CNN','Atlantic','Fox News','Guardian','National Review','NPR','Reuters', 'Vox','Washington Post'])]\n","\n","        df_articles.to_csv('real_news.csv', encoding='utf-8')\n","\n","        fake_news = pd.read_csv(\"fake.csv\")\n","        # fake_news = fake_news[:1000]   \n","        real_news = pd.read_csv(\"real_news.csv\")\n","        # real_news = real_news[:1000]  \n","\n","        \n","        real_news2 = real_news[['title', 'content', 'publication']]\n","        real_news2.dropna()\n","        real_news2['multiLabel'] = 1\n","        real_news2.head(3)\n","    \n","        fake_news2 = fake_news[['title', 'text','site_url']]\n","        fake_news2.dropna()\n","        fake_news2['multiLabel'] = 0\n","        fake_news2.head(5)\n","        \n","        # cleaning the data and add it as a column of the dataset \n","        fake_news2['clean'] = fake_news2['text'].apply(cleaning)\n","        real_news2['clean'] = real_news2['content'].apply(cleaning)\n","        print(\"After cleaning\\n\")\n","        fake_news2.head(3)\n","        real_news2.head(3)    \n","        \n","        # concatenate real and fake news data together\n","        data_news = [real_news2, fake_news2]\n","        df_news = pd.concat(data_news)[['clean', 'multiLabel']]\n","        # train, test split\n","        df_train, df_test = train_test_split(df_news, test_size=0.2, random_state=0)\n","        \n","        # Enrich with original data frame\n","        df_news_enriched = [df_train, train_news]\n","        news_dataset = pd.concat(df_news_enriched)[['clean', 'multiLabel']]\n","\n","        \n","\n","        \n","        X_test_enrich = [test_news, df_test]\n","        df_test_enrich = pd.concat(X_test_enrich)[['clean', 'multiLabel']]\n","\n","        \n","        y_test_enrich = [test_news['multiLabel'], df_test['multiLabel']]\n","        df_y_test_enrich = pd.concat(y_test_enrich)\n","        df_y_test_enrich\n","        \n","        return news_dataset, df_test_enrich, df_y_test_enrich;\n","\n","    # XGBoost enrich pipline  ***************************\n","    def xgboost_enrich_pipline(self, ngram_input_range=(1,1)):\n","        \n","        news_dataset, df_test_enrich, df_y_test_enrich = self.data_enrichment()\n","\n","        tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=ngram_input_range)\n","\n","        self.xgboost_enrich_pipeline = Pipeline([\n","            ('vectorizer', tfidf_vectorizer),\n","            ('clf', xgb.XGBClassifier(random_state=1,learning_rate=0.01))\n","            ])\n","\n","        self.xgboost_enrich_pipeline.fit(news_dataset['clean'].values, news_dataset['multiLabel'])\n","        predicts = self.xgboost_enrich_pipeline.predict(df_test_enrich['clean'].values)\n","        score = metrics.accuracy_score(df_y_test_enrich, predicts)\n","\n","        print(\"Accuracy:\",score)\n","        print(\"\\n\")\n","        print(classification_report(df_y_test_enrich, predicts))\n","        return score\n","\n","\n","    # SVM enrich pipline  ***************************\n","    def svm_enrich_pipline(self, ngram_input_range=(1,1)):\n","        \n","        news_dataset, df_test_enrich, df_y_test_enrich = self.data_enrichment()\n","\n","        tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=ngram_input_range)\n","\n","        self.svm_enrich_pipeline = Pipeline([\n","            ('vectorizer', tfidf_vectorizer),\n","            ('clf', svm.SVC(probability=True))\n","            ])\n","\n","        self.svm_enrich_pipeline.fit(news_dataset['clean'].values, news_dataset['multiLabel'])\n","        predicts = self.svm_enrich_pipeline.predict(df_test_enrich['clean'].values)\n","        score = metrics.accuracy_score(df_y_test_enrich, predicts)\n","\n","        print(\"Accuracy:\",score)\n","        print(\"\\n\")\n","        print(classification_report(df_y_test_enrich, predicts))\n","        return score\n","        \n","    # Random Forest enrich pipline  ***************************    \n","    def rf_enrich_pipline(self, ngram_input_range=(1,1)):\n","        \n","        news_dataset, df_test_enrich, df_y_test_enrich = self.data_enrichment()\n","\n","        tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=ngram_input_range)\n","\n","        self.randomForest_enrich_pipeline = Pipeline([\n","            ('vectorizer', tfidf_vectorizer),\n","            ('clf', RandomForestClassifier(n_estimators = 300))\n","            ])\n","\n","        self.randomForest_enrich_pipeline.fit(news_dataset['clean'].values, news_dataset['multiLabel'])\n","        predicts = self.randomForest_enrich_pipeline.predict(df_test_enrich['clean'].values)\n","        score = metrics.accuracy_score(df_y_test_enrich, predicts)\n","        print(\"Accuracy:\",score)\n","        print(\"\\n\")\n","        print(classification_report(df_y_test_enrich, predicts))\n","        return score\n","\n","\n","    #predict based on XGBoost\n","    def xgboost_enrich_predict(self, text):\n","        predicted = self.xgboost_enrich_pipeline.predict([text])\n","        predicedProb = self.xgboost_enrich_pipeline.predict_proba([text])[:,1]\n","        return predicted, float(predicedProb)\n","\n","    #predict based on SVM\n","    def svm_enrich_predict(self, text):\n","        predicted = self.svm_enrich_pipeline.predict([text])\n","        predicedProb = self.svm_enrich_pipeline.predict_proba([text])[:,1]\n","        return predicted, float(predicedProb)\n","\n","\n","    #predict based on Random Forest\n","    def rf_enrich_predict(self, text):\n","        predicted = self.randomForest_enrich_pipeline.predict([text])\n","        predicedProb = self.randomForest_enrich_pipeline.predict_proba([text])[:,1]\n","        \n","        return predicted, float(predicedProb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZkYE5WU2WKZ","colab_type":"text"},"source":["# 1. What are the biggest impact features for your factor? (gini index) etc. using liar liar, and adding a second dataset.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JB7aMMkbailW"},"source":["## Creating an object of CorpusStructure "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ClPtfYyEnhU4","colab":{}},"source":["corpusStructure = CorpusStructure()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zdX3OBZy7l3u","colab":{}},"source":["accuracy_dictionary_2gram = {}\n","accuracy_dictionary_3gram = {}\n","accuracy_dictionary_4gram = {}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F1dvvev-2Jvc","colab_type":"text"},"source":["# 2. compare : with columns 1-x, here are the algorithm score, adding the rest of columns in liarliar brings us to / changes to this f1, accuracy, roc/auc, polynomial"]},{"cell_type":"code","metadata":{"id":"lsqi1ONnZBJe","colab_type":"code","colab":{}},"source":["#Uni-gram\n","xgb_accuracy_2gram = corpusStructure.xgboost_pipline(ngram_input_range=(1,1))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UV8vjAvmZSdi","colab_type":"code","colab":{}},"source":["#Uni-gram\n","svm_accuracy_2gram = corpusStructure.svm_pipline(ngram_input_range=(1,1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6q3bR8DQZSpV","colab_type":"code","colab":{}},"source":["#Uni-gram\n","rfc_accuracy_2gram = corpusStructure.rf_pipline(ngram_input_range=(1,1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WqbA1HDjbXl7"},"source":["## Bi-gram"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mG_GEjOh_pkW","colab":{}},"source":["xgb_accuracy_2gram = corpusStructure.xgboost_pipline(ngram_input_range=(2,2))\n","accuracy_dictionary_2gram['XGB'] = xgb_accuracy_2gram"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Mahn_qr2ElS","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7NVgXden0k9m","colab":{}},"source":["svm_accuracy_2gram = corpusStructure.svm_pipline(ngram_input_range=(2,2))\n","accuracy_dictionary_2gram['SVM'] = svm_accuracy_2gram"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"feH4tHJGpFdB","colab":{}},"source":["rfc_accuracy_2gram = corpusStructure.rf_pipline(ngram_input_range=(2,2))\n","accuracy_dictionary_2gram['RF'] = rfc_accuracy_2gram"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jRusmOIobegU"},"source":["## Tri-gram"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t7_KmQ5X_v13","colab":{}},"source":["xgb_accuracy_3gram = corpusStructure.xgboost_pipline(ngram_input_range=(3,3))\n","accuracy_dictionary_3gram['XGB'] = xgb_accuracy_3gram"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7r_WwcUF0oHY","colab":{}},"source":["svm_accuracy_3gram = corpusStructure.svm_pipline(ngram_input_range=(3,3))\n","accuracy_dictionary_3gram['SVM'] = svm_accuracy_3gram"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Jk3GT318pFi2","colab":{}},"source":["rfc_accuracy_3gram = corpusStructure.rf_pipline(ngram_input_range=(3,3))\n","accuracy_dictionary_3gram['RF'] = rfc_accuracy_3gram"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jgSB9OBZbhOh"},"source":["## 4-gram"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"S8vzhxOB3STM","colab":{}},"source":["xgb_accuracy_4gram = corpusStructure.xgboost_pipline(ngram_input_range=(4,4))\n","accuracy_dictionary_4gram['XGB'] = xgb_accuracy_4gram"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-Erspjvz3Sod","colab":{}},"source":["svm_accuracy_4gram = corpusStructure.svm_pipline(ngram_input_range=(4,4))\n","accuracy_dictionary_4gram['SVM'] = svm_accuracy_4gram"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6Fxq5O_H3Sxd","colab":{}},"source":["rfc_accuracy_4gram = corpusStructure.rf_pipline(ngram_input_range=(4,4))\n","accuracy_dictionary_4gram['RF'] = rfc_accuracy_4gram"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8YKz6GHIbxUh"},"source":["## Comparing the results"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DIH1eOVkb2m0","colab":{}},"source":["import matplotlib.pylab as plt\n","#plt.bar(accuracy_dictionary.keys(), accuracy_dictionary.values(), color='g')\n","#print(accuracy_dictionary_2gram)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sJG6K4aW9dcb","colab":{}},"source":["lists2 = sorted(accuracy_dictionary_2gram.items()) # sorted by key, return a list of tuples\n","lists3 = sorted(accuracy_dictionary_3gram.items()) # sorted by key, return a list of tuples\n","lists4 = sorted(accuracy_dictionary_4gram.items()) # sorted by key, return a list of tuples\n","\n","\n","x2, y2 = zip(*lists2) # unpack a list of pairs into two tuples\n","x3, y3 = zip(*lists3) # unpack a list of pairs into two tuples\n","x4, y4 = zip(*lists4) # unpack a list of pairs into two tuples\n","\n","plt.plot(x2, y2, label ='Bi-gram', color='red')\n","plt.plot(x3, y3, label ='tri-gram', color='blue')\n","plt.plot(x4, y4, label = '4-gram', color='green')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aD2-jh1ybmNQ"},"source":["## Data Amalgamation - Enriching the models"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JKaNgxl7DxYE","colab":{}},"source":["accuracy_dictionary_enriched = {}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Qa0dNvPCWLfG","colab":{}},"source":["import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5EebjdTH_2Oj","colab":{}},"source":["xgb_accuracy_enriched = corpusStructure.xgboost_enrich_pipline(ngram_input_range=(4,4))\n","accuracy_dictionary_enriched['XGB'] = xgb_accuracy_enriched"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gsso_tsS0y2p","colab":{}},"source":["svm_accuracy_enriched = corpusStructure.svm_enrich_pipline(ngram_input_range=(4,4))\n","accuracy_dictionary_enriched['SVM'] = svm_accuracy_enriched"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mgSUr78SpPTh","colab":{}},"source":["rfc_accuracy_enriched = corpusStructure.rf_enrich_pipline(ngram_input_range=(4,4))\n","accuracy_dictionary_enriched['RF'] = rfc_accuracy_enriched"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KTp4lHL8C8i5"},"source":["## Comparing the results after enriching"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"47JjSBmpC8i7","colab":{}},"source":["import matplotlib.pylab as plt\n","#plt.bar(accuracy_dictionary.keys(), accuracy_dictionary.values(), color='g')\n","#print(accuracy_dictionary_2gram)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U0dzxl_1Uj6s","colab":{}},"source":["print(accuracy_dictionary_enriched)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OwDahFEfC8i9","colab":{}},"source":["lists_enriched = sorted(accuracy_dictionary_enriched.items()) # sorted by key, return a list of tuples\n","\n","x_enriched, y_enriched = zip(*lists_enriched) # unpack a list of pairs into two tuples\n","\n","plt.plot(x_enriched, y_enriched, label ='4-gram Enriched', color='red')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOxeTeXwwPKe","colab_type":"text"},"source":["# 3. compare with and without amalgamation : how does it affect confusion matrix and metrics\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H0KBtDr0wU-0","colab_type":"text"},"source":[" Data amalgamation increases the accuracy for example for the XGBoost model the acruracy increases from 20% to 54%. The table below shows the differences with and without ammalgametion:\n","\n"," ### comparing the accuracy with and without amalgamation for 4-gram:\n","\n","|Accuracy  |  Without Amalgamation |  With Amalgamation |\n","|-----|-----|-----|\n","| XGBoost                 |  0.2123 |  0.5429 |  \n","| SVM                 |  0.2036 |0.6640 |\n","| Random Forest  |  0.203  |0.6634  |\n","\n","\n"," \n"]},{"cell_type":"markdown","metadata":{"id":"zO5-hDdz2mmJ","colab_type":"text"},"source":["# 5. with only liar liar --> then derived /augmented columns --> then dataset2 --> then dataset3 : compare at each step and show how results improve (or not) \n","Dataset 1: Liar liar\n","Dataset 2: \"fake.csv\"\n","Dataset 3: \"articles1.csv\"\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9ieQjdO7jvZK","colab_type":"text"},"source":["# Get Corpus Structure Score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sChoMR_2AAQC","colab":{}},"source":["def DATAMINERS_getCorpusStructureScore_xgboost(text): \n","    multiLabelValue, probValue = corpusStructure.xgboost_enrich_predict(text)\n","    \n","    return multiLabelValue[0], 1 - float(probValue)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aUy2ADkVT3DN","colab":{}},"source":["def DATAMINERS_getCorpusStructureScore_svm(text): \n","    multiLabelValue, probValue = corpusStructure.svm_enrich_predict(text)\n","    return multiLabelValue[0], 1 - float(probValue)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dHa-T4zepPWb","colab":{}},"source":["def DATAMINERS_getCorpusStructureScore_rf(text): \n","    multiLabelValue, probValue = corpusStructure.rf_enrich_predict(text)\n","    return multiLabelValue[0], 1 - float(probValue)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6qpSWKMiJRx","colab_type":"code","colab":{}},"source":["print(DATAMINERS_getCorpusStructureScore_xgboost(\"McCain opposed a requirement that the government buy American-made motorcycles. And he said all buy-American provisions were quote 'disgraceful.\"))\n","## catagorize 'true'= 5, 'mostly-true'= 4, 'half-true'= 3,'barely-true'=2, 'false'=1, 'pants-fire'=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wf3J6eaoAMYG","colab":{}},"source":["# print(DATAMINERS_getCorpusStructureScore_xgboost(\"When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjNrYGqIjvZV","colab_type":"code","colab":{}},"source":["#!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9H44KuOymYo1","colab_type":"code","colab":{}},"source":["#type(corpusStructure.randomForest_enrich_pipeline)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tiZObxrj5WQb","colab_type":"text"},"source":["#6. How can we label the bs column and distribute with other labels in the kaggle fake news dataset? What works and what did not work? <balancing text based data set labels>\n","\n"]},{"cell_type":"code","metadata":{"id":"A2pPSwD79Dw7","colab_type":"code","colab":{}},"source":["# # Fake News Dataset column/feature names\n","fake_labels = ['uuid', 'ord_in_thread', 'author', 'published', 'title', 'text', 'language', 'crawled',\n","            'site_url', 'country', 'domain_rank', 'thread_title', 'spam_score', 'main_img_url',\n","            'replies_count', 'participants_count', 'likes', 'comments', 'shares', 'type']\n","\n","# Fake News Dataset passed to a daraframe\n","#fake_bias_data.GetContentFile('fake.csv')  \n","fake_dataset = pd.read_csv('fake.csv', error_bad_lines=False)\n","output = fake_dataset.to_csv('fake.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLWA6qKF9Z70","colab_type":"code","colab":{}},"source":["fake_dataset.head(-10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"69RfCLXz9q1x","colab_type":"code","colab":{}},"source":["fake_dataset.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUJdMgxo9vSd","colab_type":"code","colab":{}},"source":["fake_dataset['type'].value_counts().plot(kind = 'bar')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nT1zbKSW9zpN","colab_type":"code","colab":{}},"source":["fake_dataset['type'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUQCWQ8Y-SrQ","colab_type":"code","colab":{}},"source":["fake_dataset['type'].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRSbBzi1-Tnp","colab_type":"code","colab":{}},"source":["fake_dataset['type'].value_counts(normalize=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2yOIgGlY93xW","colab_type":"code","colab":{}},"source":["fake_dataset['text']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MvDM8TRl5ZzE","colab_type":"text"},"source":["# 7. Polynomial equation for multi-class classification : try diff methods : average, weighted average, transformer.\n","The following cells are trying transformers\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8ycpiIvi7KlF","colab_type":"text"},"source":["# 10. Use a Transformer in your Factor, eg, multi-class classification, or weights of attentions in your polynomial instead of accuracies.\n","The following cells are trying transformers\n"]},{"cell_type":"markdown","metadata":{"id":"ezlSuA2v5iEv","colab_type":"text"},"source":["Trying Transformers"]},{"cell_type":"markdown","metadata":{"id":"jvyOwTbsEoYS","colab_type":"text"},"source":["## Importing the dataset\n"]},{"cell_type":"code","metadata":{"id":"2DWES8VkvpRu","colab_type":"code","colab":{}},"source":["#df = pd.read_csv('train.tsv', delimiter='\\t', header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EaKWW5tqD569","colab_type":"code","colab":{}},"source":["#df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZ58RBvmD71J","colab_type":"code","colab":{}},"source":["#train_news.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"maGdYeY-D10J","colab_type":"code","colab":{}},"source":["df = train_news"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIOsYd_CvpRx","colab_type":"code","colab":{}},"source":["batch_1 = df\n","#batch_1 = train_news"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zyRBo8q2A9-n","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fSlQqdYrE0AT","colab_type":"text"},"source":["We can ask pandas how many sentences are labeled as \"false\", \"mostly-true\", \"half-true\", \"true\", \"barely true\" and \"pants-fire\""]},{"cell_type":"code","metadata":{"id":"hNXcNdQ2vpR1","colab_type":"code","colab":{}},"source":["batch_1['label'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RpGTgZ0HvpR3","colab_type":"code","colab":{}},"source":["!pip install transformers\n","!pip install torchvision \n","\n","!pip install tensorflow-gpu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJNpaglbvpR5","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","import torch\n","import transformers as ppb\n","import warnings\n","warnings.filterwarnings('ignore')\n","#from transformers import *\n","from transformers import DistilBertTokenizer, RobertaTokenizer \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eaD2j4hTFOKd","colab_type":"text"},"source":["## Loading the Pre-trained BERT model\n","Let's now load a pre-trained BERT model. "]},{"cell_type":"code","metadata":{"id":"F2yCMumqvpR7","colab_type":"code","colab":{}},"source":["# For DistilBERT:\n","model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n","\n","## Want BERT instead of distilBERT? Uncomment the following line:\n","#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n","\n","# Load pretrained model/tokenizer\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","model = model_class.from_pretrained(pretrained_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMSbQvUHFWIu","colab_type":"text"},"source":["Right now, the variable `model` holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory.\n","\n","## Model #1: Preparing the Dataset\n","Before we can hand our sentences to BERT, we need to so some minimal processing to put them in the format it requires.\n","\n","### Tokenization\n","Our first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with."]},{"cell_type":"code","metadata":{"id":"bkn5QoAmvpR8","colab_type":"code","colab":{}},"source":["tokenized = batch_1['headline_text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ditzeHrQxh2e","colab_type":"text"},"source":["### Padding\n","After tokenization, `tokenized` is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths)."]},{"cell_type":"code","metadata":{"id":"txoQR9E_w83k","colab_type":"code","colab":{}},"source":["max_len = 0\n","for i in tokenized.values:\n","    if len(i) > max_len:\n","        max_len = len(i)\n","\n","padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hyVjZxo1GDK4","colab_type":"text"},"source":["Our dataset is now in the `padded` variable, we can view its dimensions below:"]},{"cell_type":"code","metadata":{"id":"FRSRL5RIw86a","colab_type":"code","colab":{}},"source":["np.array(padded).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RnhPrpIxVgF","colab_type":"text"},"source":["### Masking\n","If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"]},{"cell_type":"code","metadata":{"id":"5xmznEDKw88_","colab_type":"code","colab":{}},"source":["attention_mask = np.where(padded != 0, 1, 0)\n","attention_mask.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WybLGHRzGfpc","colab_type":"text"},"source":["## Model #1: And Now, Deep Learning!\n","The model and inputs are ready, running the model!\n","\n","The `model()` function runs our sentences through BERT. The results of the processing will be returned into `last_hidden_states`."]},{"cell_type":"code","metadata":{"id":"JM-Up1jow9AZ","colab_type":"code","colab":{}},"source":["input_ids = torch.tensor(padded)  \n","attention_mask = torch.tensor(attention_mask)\n","\n","with torch.no_grad():\n","    last_hidden_states = model(input_ids, attention_mask=attention_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PnTmeKfCCEOI","colab_type":"code","colab":{}},"source":["features = last_hidden_states[0][:,0,:].numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptzDyITeDJw4","colab_type":"code","colab":{}},"source":["labels = batch_1[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9PFwn7GHBlm","colab_type":"text"},"source":["## Model #2: Train/Test Split\n","Let's now split our datset into a training set and testing set (even though we're using 2,000 sentences from the SST2 training set)."]},{"cell_type":"code","metadata":{"id":"cVMaAtglCERp","colab_type":"code","colab":{}},"source":["train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0sTR28Dnf5ch","colab_type":"text"},"source":["Grid Search for Parameters\n","We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength."]},{"cell_type":"code","metadata":{"id":"RVaIMPg7fw3c","colab_type":"code","colab":{}},"source":["# parameters = {'C': np.linspace(0.0001, 100, 20)}\n","# grid_search = GridSearchCV(LogisticRegression(), parameters)\n","# grid_search.fit(train_features, train_labels)\n","\n","# print('best parameters: ', grid_search.best_params_)\n","# print('best scrores: ', grid_search.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzd0HJE5QrlP","colab_type":"text"},"source":["## Evaluating Model #2\n","So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:"]},{"cell_type":"code","metadata":{"id":"iy99n1jXCEUk","colab_type":"code","colab":{}},"source":["# lr_clf = LogisticRegression()\n","# lr_clf.fit(train_features, train_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FDrlgL6MOl6C","colab":{}},"source":["xgb_clf = XGBClassifier()\n","xgb_clf.fit(train_features, train_labels)\n","y_pred_xgb = xgb_clf.predict(test_features)\n","print(\"Accuracy:\", xgb_clf.score(test_features, test_labels))\n","print(classification_report(test_labels,y_pred_xgb))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mP6I7ImjOl6F","colab":{}},"source":["rf_clf = RandomForestClassifier()\n","rf_clf.fit(train_features, train_labels)\n","y_pred_rfc = rf_clf.predict(test_features)\n","print(\"Accuracy:\", rf_clf.score(test_features, test_labels))\n","print(classification_report(test_labels,y_pred_rfc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W--xHb67Ol6H","colab":{}},"source":["svm_clf = svm.SVC()\n","svm_clf.fit(train_features, train_labels)\n","y_pred_svm = svm_clf.predict(test_features)\n","print(\"Accuracy:\", svm_clf.score(test_features, test_labels))\n","print(classification_report(test_labels,y_pred_svm))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FvrTNbvQOl6K"},"source":[" ### comparing the accuracy with and without transforming:\n","\n","|Accuracy  |  Without Transforming |  With Transforming |\n","|-----|-----|-----|\n","| XGBoost                 |  0.2123 |  0.252 |  \n","| SVM                 |  0.2036 |0.26 |\n","| Random Forest  |  0.203  |0.268  |\n"]},{"cell_type":"markdown","metadata":{"id":"ldU0rsHh69bG","colab_type":"text"},"source":["# 9. What are your conclusions on how can we improve multi-class classification on the liar liar dataset , what methods work to increase scores/metrics? (edited) \n","\n"]},{"cell_type":"markdown","metadata":{"id":"fDutxGqp7Ax-","colab_type":"text"},"source":["# 11 Extra credit: Implement a Topic Naming Solution on one of the LDAs from your dataset \n","\n"]},{"cell_type":"code","metadata":{"id":"EvC0fkfrTCkL","colab_type":"code","colab":{}},"source":["#LDA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fAz79W84B_-X","colab_type":"code","colab":{}},"source":["train_news['index'] = train_news.index\n","data = train_news\n","train_lda = data[['clean', 'index']]\n","train_lda.head(4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8fk02u8kMYsn","colab_type":"code","colab":{}},"source":["test_news['index'] = test_news.index\n","data = test_news\n","test_lda = data[['clean', 'index']]\n","test_lda.head(4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Am3ru3OGMY1w","colab_type":"code","colab":{}},"source":["valid_news['index'] = valid_news.index\n","data = valid_news\n","valid_lda = data[['clean', 'index']]\n","valid_lda.head(4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcrL3NFKMY5B","colab_type":"code","colab":{}},"source":["processed_docs = train_lda['clean'].map(lambda doc: doc.split(\" \"))\n","processed_docs[:4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MM18r6bgMqP0","colab_type":"code","colab":{}},"source":["import gensim\n","def get_word_tokens(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if len(token) > 3:\n","            result.append(token)\n","    return result\n","tokenized_docs_local = train_news['clean'].map(get_word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bU3NieR1MqVq","colab_type":"code","colab":{}},"source":["def get_dictionary_print_words(dataframe,colname):\n","    dictionary_gensim = gensim.corpora.Dictionary(processed_docs)\n","    count = 0\n","    print('######## DICTIONARY Words and occurences ########')\n","    for k, v in dictionary_gensim.iteritems():\n","        print(k, v)\n","        count += 1\n","        if count > 10:\n","            break\n","    dictionary_gensim.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n","    return dictionary_gensim, tokenized_docs_local"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpJSAzhxMqYn","colab_type":"code","colab":{}},"source":["def get_bow_corpus_print_sample(dataframe,colname):\n","    dictionary_gensim, tokenized_docs_local = get_dictionary_print_words(dataframe, colname)\n","    bow_corpus_local = [dictionary_gensim.doc2bow(doc) for doc in tokenized_docs_local]\n","    bow_doc_local_0 = bow_corpus_local[0]\n","    print('\\n ######## BOW VECTOR FIRST ITEM ########')\n","    print(bow_doc_local_0)\n","    print('\\n ######## PREVIEW BOW ########')\n","    for i in range(len(bow_doc_local_0)):\n","        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_local_0[i][0], \n","                                               dictionary_gensim[bow_doc_local_0[i][0]], bow_doc_local_0[i][1]))\n","    return bow_corpus_local, dictionary_gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzxjGir8MqTC","colab_type":"code","colab":{}},"source":["def get_tfidf_corpus_print_sample(bow_corpus_local):\n","    from gensim import corpora, models\n","    tfidf = models.TfidfModel(bow_corpus_local)\n","    tfidf_corpus_local = tfidf[bow_corpus_local]\n","    print('\\n ######## TFIDF VECTOR FIRST ITEM ########')\n","    \n","    from pprint import pprint\n","    for doc in tfidf_corpus_local:\n","        pprint(doc)\n","        break\n","    return tfidf_corpus_local"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J8Ap5C09d9jD","colab_type":"text"},"source":["## Running ldamodel "]},{"cell_type":"code","metadata":{"id":"WMo5uuhGMY8H","colab_type":"code","colab":{}},"source":["def get_lda_model_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n","    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2)\n","    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=10,formatted=False)\n","    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n","\n","    #Below Code Prints Topics and Words\n","    for topic,words in lda_topics_words:\n","        print(str(topic)+ \"::\"+ str(words))\n","    return lda_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40ygQwpseMAy","colab_type":"code","colab":{}},"source":["def get_lda_model_topics_topwords_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n","    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2, random_state=1)\n","    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=10,formatted=False)\n","    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n","\n","    #Below Code Prints Topics and Words\n","    for topic,words in lda_topics_words:\n","        print(str(topic)+ \"::\"+ str(words))\n","    return lda_model,lda_topics_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gUM8-AXgeMDp","colab_type":"code","colab":{}},"source":["def identify_topic_number_score_label_topwords(text,dictionary_local,lda_model_local,lda_topics_top_words_local):\n","    bow_vector_local = dictionary_local.doc2bow(get_word_tokens(text))\n","    topic_number_local, topic_score_local = sorted(\n","        lda_model_local[bow_vector_local], key=lambda tup: -1*tup[1])[0]\n","    #print (topic_number_local, topic_score_local)\n","    return pd.Series([topic_number_local, topic_score_local,\" \".join(lda_topics_top_words_local[int(topic_number_local)][1])])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hVJDchx1eMIr","colab_type":"code","colab":{}},"source":["def update_lda_results_to_dataset(dataframe,topiccolnames,coltoapplylda,colnamedictionary,colnameldamodel, colnameldatopwords):\n","    dataframe[topiccolnames] = dataframe.apply(\n","    lambda row: identify_topic_number_score_label_topwords(\n","        row[coltoapplylda],colnamedictionary,colnameldamodel,\n","        colnameldatopwords), axis=1)\n","    return dataframe"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9u5hJz1emiE","colab_type":"text"},"source":["## Creating Bag of Words"]},{"cell_type":"code","metadata":{"id":"UU2AVRTEeML9","colab_type":"code","colab":{}},"source":["bow_corpus_headline, dictionary_headline = get_bow_corpus_print_sample(train_news,\n","                                                                      'clean')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aGGfPt61eMG4","colab_type":"code","colab":{}},"source":["lda_model_headline, lda_headline_topic_words = get_lda_model_topics_topwords_print_top_topics(bow_corpus_headline, 10 ,dictionary_headline)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YN-fMRupe-yX","colab_type":"text"},"source":["Generating TF-IDF bow_corpus:"]},{"cell_type":"code","metadata":{"id":"KdPUSqJLe4rg","colab_type":"code","colab":{}},"source":["tfidf_corpus_headline = get_tfidf_corpus_print_sample(bow_corpus_headline)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOtMJG6ce4ul","colab_type":"code","colab":{}},"source":["lda_tfidf_model_headline  = get_lda_model_print_top_topics(tfidf_corpus_headline,10,dictionary_headline)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5zRFYBMe40w","colab_type":"code","colab":{}},"source":["semisupervised_topic_labels = ['topic0','topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlLnFRFke46J","colab_type":"code","colab":{}},"source":["headlinetopiccolnames = ['topic_number','lda_score','topic_top_words']\n","train_news = update_lda_results_to_dataset(\n","    train_news, headlinetopiccolnames,'clean', dictionary_headline, lda_model_headline, lda_headline_topic_words)\n","train_news.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyXOEANge44C","colab_type":"code","colab":{}},"source":["test_news = update_lda_results_to_dataset(\n","    test_news,headlinetopiccolnames,'clean',\n","  dictionary_headline,lda_model_headline,lda_headline_topic_words)\n","test_news.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z41B4deLskdV","colab_type":"code","colab":{}},"source":["import seaborn as sb\n","def create_distribution(dataFile):\n","    g = sb.countplot(x='topic_number', data=dataFile, palette='hls')\n","    g.set_xticklabels(g.get_xticklabels(),rotation=90)\n","\n","    return g\n","\n","create_distribution(train_news) # TRAIN Document Vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37MwKxCOs62_","colab_type":"code","colab":{}},"source":["create_distribution(test_news)# TEST Document Vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YREiSV3Pe4yE","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","vectorizer = CountVectorizer(analyzer='word',       \n","                             min_df=10,\n","                             stop_words='english',             \n","                             lowercase=True,                   \n","                             token_pattern='[a-zA-Z0-9]{3,}',  \n","                            max_features=50000,)\n","data_vectorized = vectorizer.fit_transform(train_news['clean'])\n","# apply transformation\n","tf = vectorizer.fit_transform(train_news['clean']).toarray()\n","# tf_feature_names tells us what word each column in the matric represents\n","tf_feature_names = vectorizer.get_feature_names()\n","\n","number_of_topics = 10\n","\n","lda_model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n","lda_model.fit(tf)\n","# Best Model\n","#lda_model = model.best_estimator_\n","# Model Parameters\n","#print(\"Best Model's Params: \", model.best_params_)\n","# Log Likelihood Score\n","#print(\"Best Log Likelihood Score: \", model.best_score_)\n","# Perplexity\n","print(\"Model Perplexity: \", lda_model.perplexity(data_vectorized))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHMaEnVA0sWe","colab_type":"code","colab":{}},"source":["def display_topics(model, feature_names, no_top_words):\n","    topic_dict = {}\n","    for topic_idx, topic in enumerate(model.components_):\n","        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n","                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n","        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n","                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n","    return pd.DataFrame(topic_dict)\n","no_top_words = 10\n","display_topics(lda_model, tf_feature_names, no_top_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iq9v3c_z0uWC","colab_type":"code","colab":{}},"source":["# Create Document — Topic Matrix\n","lda_output = lda_model.transform(data_vectorized)\n","# column names\n","topicnames = ['Topic' + str(i) for i in range(lda_model.n_components)]\n","# index names\n","docnames = [i for i in range(len(train_news))]\n","# Make the pandas dataframe\n","df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n","# Get dominant topic for each document\n","dominant_topic = np.argmax(df_document_topic.values, axis=1)\n","df_document_topic['dominant_topic'] = dominant_topic\n","# Styling\n","def color_green(val):\n"," color = 'green' if val > .1 else 'black'\n"," return 'color: {col}'.format(col=color)\n","def make_bold(val):\n"," weight = 700 if val > .1 else 400\n"," return 'font-weight: {weight}'.format(weight=weight)\n","# Apply Style\n","df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n","df_document_topics\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlIz07KR08Qu","colab_type":"code","colab":{}},"source":["# Topic-Keyword Matrix\n","df_topic_keywords = pd.DataFrame(lda_model.components_)\n","\n","# Assign Column and Index\n","df_topic_keywords.columns = vectorizer.get_feature_names()\n","df_topic_keywords.index = topicnames\n","\n","# View\n","df_topic_keywords.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jrk2vT7M3H3w","colab_type":"code","colab":{}},"source":["combined_train = train_news.drop(['clean','headline_text','jsonid','subject',\t'speaker',\t'speakerjobtitle',\t'stateinfo',\t'partyaffiliation','context'], axis=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0NhuTLCD6e9","colab_type":"code","colab":{}},"source":["# Change Dataset values to numeric\n","from sklearn.preprocessing import LabelEncoder\n","lb = LabelEncoder() \n","combined_train['label'] = lb.fit_transform(combined_train['label'])\n","combined_train['topic_top_words'] = lb.fit_transform(combined_train['topic_top_words'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cWIY9DBb0_3o","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","\n","lda = pd.merge(combined_train,df_document_topic,left_index =True ,right_index=True)\n","X_train, X_test, y_train, y_test = train_test_split(lda.drop(['label'], axis=1),lda['label'], test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGEmtu0oDzcc","colab_type":"code","colab":{}},"source":["def run_models(X_train, y_train, X_test, y_test):\n","    models = []\n","    models.append(('RandomForest', RandomForestClassifier(n_estimators=10)))\n","    models.append(('SVM', SVC(gamma='auto')))\n","    models.append(('XGBoost', XGBClassifier(random_state=1,learning_rate=0.01, early_stopping_rounds=10)))\n","    for name, model in models:\n","        model.fit(X_train,y_train)\n","        print(name, ': ')\n","        # make predictions for test data\n","        y_pred = model.predict(X_test)\n","        predictions = [round(value) for value in y_pred]\n","\n","        # evaluate predictions\n","        from sklearn.metrics import classification_report\n","        print(classification_report(y_test,predictions))\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4rdMCUtN4KFA","colab_type":"code","colab":{}},"source":["# scale data using StandardScaler\n","# from sklearn.preprocessing import StandardScaler\n","# from sklearn.neighbors import KNeighborsClassifier\n","# from sklearn.tree import DecisionTreeClassifier\n","# from sklearn.svm import SVC\n","\n","# scaler = StandardScaler()\n","# lda_scaled_train = scaler.fit_transform(X_train)\n","# lda_scaled_test = scaler.fit_transform(X_test)\n","# lda_scaled = scaler.fit_transform(lda.drop(['label'], axis=1))\n","# run_models(lda_scaled_train,y_train, lda_scaled_test,y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eUJDz0sdSU3a","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2zNYN-dG36_","colab_type":"code","colab":{}},"source":["xgb_clf = XGBClassifier()\n","eval_set = [(X_test, y_test)]\n","xgb_clf.fit(X_train, y_train, early_stopping_rounds=10, eval_set=eval_set)\n","y_pred_xgb = xgb_clf.predict(X_test)\n","print(\"Accuracy:\", xgb_clf.score(X_test, y_test))\n","print(classification_report(y_test,y_pred_xgb))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQM8qCdPwQcS","colab_type":"code","colab":{}},"source":["rf_clf = RandomForestClassifier()\n","# X_test = X_test.fillna(X_train.mean())\n","# X_train = X_test.fillna(X_train.mean())\n","rf_clf.fit(X_train, y_train)\n","y_pred_rfc = rf_clf.predict(X_test)\n","print(\"Accuracy:\", rf_clf.score(X_test, y_test))\n","print(classification_report(y_test,y_pred_rfc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bBfm6wpxRj-t","colab_type":"code","colab":{}},"source":["svm_clf = svm.SVC()\n","svm_clf.fit(X_train, y_train)\n","y_pred_svm = svm_clf.predict(X_test)\n","print(\"Accuracy:\", svm_clf.score(X_test, y_test))\n","print(classification_report(y_test,y_pred_svm))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rrt9Ec_Yc0jY","colab_type":"text"},"source":[" ### comparing the accuracy with and without LDA:\n","\n","|Accuracy  |  Without LDA |  With LDA |\n","|-----|-----|-----|\n","| XGBoost                 |  0.2123 |  0.446 |  \n","| SVM                 |  0.2036 |0.218 |\n","| Random Forest  |  0.203  |0.425  |\n"]},{"cell_type":"code","metadata":{"id":"hCSJr3J1Kcof","colab_type":"code","colab":{}},"source":["# unexpected result for svm after LDA"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lDPEDISDvCcs","colab_type":"text"},"source":[" ### Concluding the comparision of the accuracy with and without different situations:\n","\n","\n","|Accuracy  |  Without Anythnig |  With Transforming |  With LDA |  With Amalgamation |\n","|-----|-----|-----|-----|-----|\n","| XGBoost                 |  0.2123 |  0.252 |  0.446 |    0.5429 |\n","| SVM                 |  0.2036 |0.26 |0.218 |0.6640 |\n","| Random Forest  |  0.203  |0.268  |0.425  |0.6634  |\n","\n"]}]}